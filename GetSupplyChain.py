#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GetSupplyChain.py
Version: 2.0
Description: Downloads Taiwan stock market supply chain data from ic.tpex.org.tw
             Creates separate CSV files for each industry chain with upstream/downstream companies
"""

import requests
from bs4 import BeautifulSoup
import csv
import time
import re
import urllib3
from datetime import datetime

# Disable SSL warnings (ic.tpex.org.tw has certificate issues)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://ic.tpex.org.tw"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
VERIFY_SSL = False

# Global mapping for foreign companies
FOREIGN_COMPANY_MAP = {}

def load_foreign_company_map():
    """è¼‰å…¥å¤–åœ‹ä¼æ¥­å°ç…§è¡¨"""
    global FOREIGN_COMPANY_MAP
    import os
    filepath = 'data/raw_SupplyChain-non-TWSE-TPEX.csv'
    if os.path.exists(filepath):
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['åç¨±'] and row['è‚¡ç¥¨ä»£è™Ÿ']:
                    FOREIGN_COMPANY_MAP[row['åç¨±']] = row['è‚¡ç¥¨ä»£è™Ÿ']
        print(f"  è¼‰å…¥ {len(FOREIGN_COMPANY_MAP)} ç­†å¤–åœ‹ä¼æ¥­å°ç…§")

def get_chain_data(chain_code):
    """å–å¾—ç”¢æ¥­éˆçš„å®Œæ•´è³‡æ–™ï¼ˆå­åˆ†é¡žã€ä½ç½®ã€å…¬å¸æ¸…å–®ï¼‰"""
    url = f"{BASE_URL}/introduce.php?ic={chain_code}"
    try:
        print(f"æ­£åœ¨ä¸‹è¼‰ {chain_code} ç”¢æ¥­éˆè³‡æ–™...")
        response = requests.get(url, headers=HEADERS, timeout=60, verify=VERIFY_SSL)
        response.raise_for_status()
        response.encoding = 'utf-8'
        content = response.text
        soup = BeautifulSoup(content, 'html.parser')

        # Get chain name from title
        title = soup.find('title')
        chain_name = ''
        if title:
            title_text = title.get_text(strip=True)
            if '>' in title_text:
                chain_name = title_text.split('>')[-1].strip()

        # Find positions of ä¸Šæ¸¸, ä¸­æ¸¸, ä¸‹æ¸¸ sections
        upstream_pos = content.find('chain-title-panel">ä¸Šæ¸¸')
        midstream_pos = content.find('chain-title-panel">ä¸­æ¸¸')
        downstream_pos = content.find('chain-title-panel">ä¸‹æ¸¸')

        # Build subcategory info from ic_link elements
        subcategory_info = {}
        ic_links = re.findall(
            r'id="ic_link_([A-Z0-9]+)"[^>]*class="company-chain-panel"[^>]*>([^<]+)<',
            content
        )

        for code, name in ic_links:
            pos = content.find(f'ic_link_{code}')
            if midstream_pos > 0:
                if pos < midstream_pos:
                    position = 'ä¸Šæ¸¸'
                elif downstream_pos > 0 and pos < downstream_pos:
                    position = 'ä¸­æ¸¸'
                else:
                    position = 'ä¸‹æ¸¸'
            else:
                position = 'ä¸Šæ¸¸' if downstream_pos < 0 or pos < downstream_pos else 'ä¸‹æ¸¸'

            subcategory_info[code] = {
                'name': name.strip(),
                'position': position
            }

        # Extract companies from companyList divs using BeautifulSoup
        results = []
        seen_combinations = set()  # Avoid duplicate entries

        company_divs = soup.find_all('div', id=re.compile(r'^companyList_'))
        for div in company_divs:
            div_id = div.get('id', '')
            subcat_code = div_id.replace('companyList_', '')

            if subcat_code not in subcategory_info:
                continue

            subcat = subcategory_info[subcat_code]

            # Get all company links from ALL tables in this div (including foreign companies)
            company_links = div.find_all('a', href=True)
            for link in company_links:
                href = link.get('href', '')
                stock_name = link.get('title', link.get_text(strip=True))

                # Skip empty names or navigation links
                if not stock_name or stock_name in ['æ›´å¤š', '...']:
                    continue

                # Check if it's a Taiwan stock (has stk_code)
                match = re.search(r'stk_code=(\d+)', href)
                if match:
                    stock_code = match.group(1)
                else:
                    # Foreign company - use empty code
                    stock_code = ''

                # Create unique key to avoid duplicates (use name for foreign companies)
                key = f"{subcat_code}:{stock_code or stock_name}"
                if key in seen_combinations:
                    continue
                seen_combinations.add(key)

                results.append({
                    'chain_code': chain_code,
                    'chain_name': chain_name,
                    'position': subcat['position'],
                    'subcategory': subcat['name'],
                    'stock_code': stock_code,
                    'stock_name': stock_name
                })

        print(f"  âœ… æ‰¾åˆ° {len(subcategory_info)} å€‹å­åˆ†é¡ž, {len(results)} ç­†å…¬å¸è³‡æ–™")
        return results, chain_name

    except Exception as e:
        print(f"  âŒ å–å¾— {chain_code} ç”¢æ¥­éˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return [], ''

def export_chain_csv(data, chain_code, chain_name, output_path):
    """åŒ¯å‡ºç”¢æ¥­éˆ CSV"""
    fieldnames = ['ä½ç½®', 'å­åˆ†é¡ž', 'ä»£è™Ÿ', 'åç¨±']

    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        for row in data:
            # Use foreign company map if stock_code is empty
            stock_code = row['stock_code']
            if not stock_code and row['stock_name'] in FOREIGN_COMPANY_MAP:
                stock_code = FOREIGN_COMPANY_MAP[row['stock_name']]

            writer.writerow({
                'ä½ç½®': row['position'],
                'å­åˆ†é¡ž': row['subcategory'],
                'ä»£è™Ÿ': stock_code,
                'åç¨±': row['stock_name']
            })

    print(f"  ðŸ“„ å·²åŒ¯å‡ºè‡³ {output_path} ({len(data)} ç­†)")

def build_supply_chain_map(chain_names):
    """å¾žç”¢æ¥­éˆ CSV æª”æ¡ˆå»ºç«‹ SupplyChainMap.csv"""
    import os

    # Read all supply chain CSVs
    all_chain_data = {}
    for filename in os.listdir('data'):
        if filename.startswith('raw_SupplyChain_') and filename.endswith('.csv') and filename != 'raw_SupplyChainMap.csv':
            chain_code = filename.replace('raw_SupplyChain_', '').replace('.csv', '')
            all_chain_data[chain_code] = []
            with open(f'data/{filename}', 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    all_chain_data[chain_code].append(row)

    # Read watchlist
    watchlist = []
    with open('StockID_TWSE_TPEX.csv', 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            code = row['ä»£è™Ÿ'].strip()
            name = row['åç¨±'].strip()
            if code and code != '0000':
                watchlist.append({'code': code, 'name': name})

    print(f"\nå»ºç«‹ SupplyChainMap.csv ({len(watchlist)} å®¶è§€å¯Ÿåå–®å…¬å¸)...")

    # Build supply chain map
    results = []
    for company in watchlist:
        code = company['code']
        name = company['name']

        for chain_code, chain_data in all_chain_data.items():
            company_entries = [d for d in chain_data if d['ä»£è™Ÿ'] == code]
            if not company_entries:
                continue

            positions = list(set(e['ä½ç½®'] for e in company_entries))
            subcategories = list(set(e['å­åˆ†é¡ž'] for e in company_entries))

            upstream_companies = []
            downstream_companies = []

            for pos in positions:
                if pos == 'ä¸Šæ¸¸':
                    for d in chain_data:
                        if d['ä½ç½®'] in ['ä¸­æ¸¸', 'ä¸‹æ¸¸'] and d['ä»£è™Ÿ'] != code:
                            downstream_companies.append(f"{d['ä»£è™Ÿ']}|{d['åç¨±']}")
                elif pos == 'ä¸­æ¸¸':
                    for d in chain_data:
                        if d['ä½ç½®'] == 'ä¸Šæ¸¸' and d['ä»£è™Ÿ'] != code:
                            upstream_companies.append(f"{d['ä»£è™Ÿ']}|{d['åç¨±']}")
                        elif d['ä½ç½®'] == 'ä¸‹æ¸¸' and d['ä»£è™Ÿ'] != code:
                            downstream_companies.append(f"{d['ä»£è™Ÿ']}|{d['åç¨±']}")
                elif pos == 'ä¸‹æ¸¸':
                    for d in chain_data:
                        if d['ä½ç½®'] in ['ä¸Šæ¸¸', 'ä¸­æ¸¸'] and d['ä»£è™Ÿ'] != code:
                            upstream_companies.append(f"{d['ä»£è™Ÿ']}|{d['åç¨±']}")

            upstream_companies = list(dict.fromkeys(upstream_companies))[:20]
            downstream_companies = list(dict.fromkeys(downstream_companies))[:20]

            results.append({
                'ä»£è™Ÿ': code,
                'åç¨±': name,
                'ç”¢æ¥­éˆä»£ç¢¼': chain_code,
                'ç”¢æ¥­éˆåç¨±': chain_names.get(chain_code, chain_code),
                'ä½ç½®': '/'.join(positions),
                'å­åˆ†é¡ž': ';'.join(subcategories[:5]),
                'ä¸Šæ¸¸å…¬å¸': ';'.join(upstream_companies),
                'ä¸‹æ¸¸å…¬å¸': ';'.join(downstream_companies),
            })

    # Write CSV
    output_path = 'data/raw_SupplyChainMap.csv'
    fieldnames = ['ä»£è™Ÿ', 'åç¨±', 'ç”¢æ¥­éˆä»£ç¢¼', 'ç”¢æ¥­éˆåç¨±', 'ä½ç½®', 'å­åˆ†é¡ž', 'ä¸Šæ¸¸å…¬å¸', 'ä¸‹æ¸¸å…¬å¸']
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"  ðŸ“„ å·²åŒ¯å‡º {len(results)} ç­†è¨˜éŒ„è‡³ {output_path}")

def main():
    import sys
    import os
    sys.stdout.reconfigure(encoding='utf-8')

    # Ensure data directory exists
    os.makedirs('data', exist_ok=True)

    print("=" * 60)
    print(f"ç”¢æ¥­éˆè³‡æ–™ä¸‹è¼‰ç¨‹å¼ v2.1")
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # Load foreign company mapping
    load_foreign_company_map()

    # Industry chains to download
    chains = {
        'F000': 'é›»è…¦åŠé€±é‚Šè¨­å‚™',
        'I000': 'é€šä¿¡ç¶²è·¯',
        '5300': 'äººå·¥æ™ºæ…§',
        '5800': 'é‹å‹•ç§‘æŠ€',
        'D000': 'åŠå°Žé«”',
        'U000': 'é‡‘èž',
        'T000': 'äº¤é€šé‹è¼¸åŠèˆªé‹',
        'B000': 'ä¼‘é–’å¨›æ¨‚',
        'R000': 'è»Ÿé«”æœå‹™',
        'C100': 'è£½è—¥',
        '5500': 'è³‡é€šè¨Šå®‰å…¨',
        'V000': 'è²¿æ˜“ç™¾è²¨',
        'R300': 'é›»å­å•†å‹™',
        '5200': 'é‡‘èžç§‘æŠ€',
        'L000': 'å°åˆ·é›»è·¯æ¿',
        'C200': 'é†«ç™‚å™¨æ',
        'M000': 'é£Ÿå“',
        'X000': 'å…¶ä»–',
        '6000': 'è‡ªå‹•åŒ–',
        'G000': 'å¹³é¢é¡¯ç¤ºå™¨',
        'P000': 'é›»æ©Ÿæ©Ÿæ¢°',
    }

    for chain_code, expected_name in chains.items():
        print(f"\nè™•ç† {chain_code} ({expected_name})...")

        data, chain_name = get_chain_data(chain_code)
        time.sleep(2)  # Rate limiting

        if data:
            output_path = f"data/raw_SupplyChain_{chain_code}.csv"
            export_chain_csv(data, chain_code, chain_name, output_path)
        else:
            print(f"  âš ï¸ {chain_code} ç„¡è³‡æ–™")

    # Build SupplyChainMap.csv from the chain CSVs
    build_supply_chain_map(chains)

    print("\n" + "=" * 60)
    print("è™•ç†å®Œæˆ! ðŸŽ‰")

if __name__ == "__main__":
    main()
